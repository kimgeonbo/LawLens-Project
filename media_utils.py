import easyocr
import whisper
import warnings
import os
import torch
import numpy as np
import streamlit as st
import config

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

try:
    from pyannote.audio import Pipeline
    PYANNOTE_AVAILABLE = True
except ImportError:
    PYANNOTE_AVAILABLE = False
# ë°ì½”ë ˆì´ì…˜ ì‚¬ìš©ìœ¼ë¡œ ì²˜ìŒ í•œë²ˆë§Œ ë¡œë“œ
@st.cache_resource(show_spinner=False)
def load_easyocr_reader():
    # EasyOCR ëª¨ë¸ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    return easyocr.Reader(config.OCR_LANGUAGES, gpu=torch.cuda.is_available(), verbose=False)

@st.cache_resource(show_spinner=False)
def load_whisper_model():
    # Whisper ëª¨ë¸ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    return whisper.load_model(config.WHISPER_MODEL_SIZE)

@st.cache_resource(show_spinner=False)
def load_pyannote_pipeline(hf_token):
    # Pyannote í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    if not PYANNOTE_AVAILABLE or not hf_token:
        return None
    
    try:
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )
        if torch.cuda.is_available():
            pipeline.to(torch.device("cuda"))
        return pipeline
    except Exception as e:
        print(f"Pyannote ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def group_text_by_line(results, y_threshold=15):
    """
    EasyOCR ê²°ê³¼(ë‹¨ì–´ ë‹¨ìœ„)ë¥¼ Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ë¬¶ì–´ì„œ 'ì¤„(Line)' ë‹¨ìœ„ë¡œ ë§Œë“¦.
    ì±„íŒ…ë°©ì€ [ë‹‰ë„¤ì„] [ë‚´ìš©]ì´ ê°™ì€ ë†’ì´ì— ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ í•©ì³ì•¼ í•¨.
    """
    if not results:
        return ""

    # Yì¢Œí‘œ(ì„¸ë¡œ ìœ„ì¹˜) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    # item êµ¬ì¡°: ([[x,y], ...], 'text', conf)
    sorted_results = sorted(results, key=lambda x: x[0][0][1]) 

    lines = []
    current_line = [sorted_results[0]]

    for i in range(1, len(sorted_results)):
        prev_y = current_line[-1][0][0][1] # ì´ì „ ê¸€ì Yì¢Œí‘œ
        curr_y = sorted_results[i][0][0][1] # í˜„ì¬ ê¸€ì Yì¢Œí‘œ

        # ë†’ì´ ì°¨ì´ê°€ í¬ì§€ ì•Šìœ¼ë©´ ê°™ì€ ì¤„ë¡œ ê°„ì£¼
        if abs(curr_y - prev_y) < y_threshold:
            current_line.append(sorted_results[i])
        else:
            # ì¤„ ë°”ê¿ˆ ë°œìƒ -> ì €ì¥ëœ ì¤„ì„ Xì¢Œí‘œ(ê°€ë¡œ) ìˆœìœ¼ë¡œ ì •ë ¬ í›„ í•©ì¹¨
            current_line.sort(key=lambda x: x[0][0][0])
            line_text = " ".join([item[1] for item in current_line])
            lines.append(line_text)
            current_line = [sorted_results[i]]

    # ë§ˆì§€ë§‰ ì¤„ ì²˜ë¦¬
    if current_line:
        current_line.sort(key=lambda x: x[0][0][0])
        lines.append(" ".join([item[1] for item in current_line]))

    return "\n".join(lines)

def extract_text_from_image(image_path):
    """
    [EasyOCR] ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    print(f"ğŸ“· ì´ë¯¸ì§€ ë¶„ì„ ì¤‘... ({image_path})")
    try:
        # GPUê°€ ì—†ìœ¼ë©´ gpu=Falseë¡œ ìë™ ì„¤ì •ë¨
        reader = load_easyocr_reader()
        result = reader.readtext(image_path, detail=1)

        return group_text_by_line(result)
    except Exception as e:
        return f"OCR ì—ëŸ¬: {str(e)}"
    
def format_time(seconds):
    # ì´ˆ ë‹¨ìœ„ë¥¼ mm:ss í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    m, s = divmod(seconds, 60)
    return f"{int(m):02d}:{int(s):02d}"

def extract_text_from_audio(audio_path, hf_token=None):
    """
    [Whisper + Pyannote] ìŒì„± -> í…ìŠ¤íŠ¸ (í™”ì ë¶„ë¦¬ í¬í•¨)
    hf_token: Hugging Face í† í° (pyannote ì‚¬ìš© ì‹œ í•„ìˆ˜)
    """
    hf_token = hf_token or config.HF_TOKEN

    print(f"ğŸ¤ ìŒì„± ë¶„ì„ ì¤‘... ({audio_path})")
    
    if not os.path.exists(audio_path):
        return "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

    try:
        # 1. ìºì‹±ëœ Whisper ëª¨ë¸ ë¡œë“œ ë° ìŒì„± ì¸ì‹ ìˆ˜í–‰
        model = load_whisper_model()
        
        transcription = model.transcribe(audio_path, language="ko")
        segments = transcription["segments"]

        pipeline = load_pyannote_pipeline(hf_token)

        if pipeline is None:
            if not PYANNOTE_AVAILABLE:
                msg = "Pyannote.audio ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜"
            elif not hf_token:
                msg = "Hugging Face í† í° ëˆ„ë½"
            else:
                msg = "íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨"
            
            print(msg)
            result_text = []
            for seg in segments:
                start = format_time(seg['start'])
                end = format_time(seg['end'])
                result_text.append(f"[{start} - {end}] {seg['text']}")
            return "\n".join(result_text)
        
        # 2. í™”ì ë¶„ë¦¬ ìˆ˜í–‰
        print("ğŸ—£ï¸ í™”ì ë¶„ë¦¬(Diarization) ìˆ˜í–‰ ì¤‘...")
        diarization = pipeline(audio_path)
        
        # 3. Whisper ì„¸ê·¸ë¨¼íŠ¸ì™€ í™”ì ì •ë³´ ë§¤ì¹­
        final_output = []
        
        for seg in segments:
            w_start, w_end, w_text = seg['start'], seg['end'], seg['text']

            speaker_counts = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                overlap_start = max(w_start, turn.start)
                overlap_end = min(w_end, turn.end)
                duration = max(0, overlap_end - overlap_start)
                
                if duration > 0:
                    speaker_counts[speaker] = speaker_counts.get(speaker, 0) + duration
            
            # ê°€ì¥ ì˜¤ë˜ ë§í•œ í™”ì ì„ íƒ (ì—†ìœ¼ë©´ Unknown)
            if speaker_counts:
                best_speaker = max(speaker_counts, key=speaker_counts.get)
            else:
                best_speaker = "Unknown"

            # í™”ì ì´ë¦„ ë³€ê²½ (SPEAKER_00 -> í™”ì A)
            speaker_label = f"í™”ì {int(best_speaker.split('_')[-1]) + 1}" if "SPEAKER" in best_speaker else best_speaker
            
            time_str = f"[{format_time(w_start)}]"
            final_output.append(f"{time_str} {speaker_label}: {w_text}")

        return "\n".join(final_output)

    except Exception as e:
        return f"ìŒì„± ë¶„ì„ ì—ëŸ¬: {str(e)}"

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš©
    print("ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ")